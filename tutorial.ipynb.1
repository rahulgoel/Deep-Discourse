{
 "metadata": {
  "name": "tutorial"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": [
      "Getting Started with CoNLL 2015 Shared Task Data", 
      "===================================", 
      "We believe that the json format of the data makes the development faster and easier to debug. ", 
      "In this tutorial, we will demonstrate how to work with these json datasets.", 
      "It is a long document, but you will want to read it. Believe me.", 
      "And by the way, the data used in this tutorial and this tutorial itself can be downloaded at www.github.com/attapol/conll15st", 
      "", 
      "The Penn Discourse Treebank Data", 
      "-----------------------------------", 
      "Each line in the file is a json line. In Python, you can turn it into a dictionary. (Similarly, you can turn it into HashMap in Java)", 
      "", 
      "The dictionary describes the following component of a relation:", 
      "", 
      "* `Arg1` : the text span of Arg1 of the relation", 
      "* `Arg2` : the text span of Arg2 of the relation", 
      "* `Connective` : the text span of the connective of the relation", 
      "* `DocID` : document id where the relation is in.", 
      "* `ID` : the relation id, which is unique across training, dev, and test sets.", 
      "* `Sense` : the sense of the relation ", 
      "* `Type` : the type of relation (Explicit, Implicit, Entrel, AltLex, or NoRel)", 
      "", 
      "The text span is in the same format for `Arg1`, `Arg2`, and `Connective`. A text span has the following fields:", 
      "", 
      "* `CharacterSpanList` : the list of character offsets (beginning, end) in the raw untokenized data file. ", 
      "* `RawText` : the raw untokenized text of the span", 
      "* `TokenList` : the list of the addresses of the tokens in the form of ", 
      "(character offset begin, character offset end, token offset within the document, sentence offset, token offset within the sentence)", 
      "", 
      "For example, "
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import json", 
      "pdtb_file = open('pdtb_trial_data.json')", 
      "relations = [json.loads(x) for x in pdtb_file];", 
      "example_relation = relations[10]", 
      "example_relation"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 1, 
       "text": [
        "{u'Arg1': {u'CharacterSpanList': [[2493, 2517]],", 
        "  u'RawText': u'and told them to cool it',", 
        "  u'TokenList': [[2493, 2496, 465, 15, 8],", 
        "   [2497, 2501, 466, 15, 9],", 
        "   [2502, 2506, 467, 15, 10],", 
        "   [2507, 2509, 468, 15, 11],", 
        "   [2510, 2514, 469, 15, 12],", 
        "   [2515, 2517, 470, 15, 13]]},", 
        " u'Arg2': {u'CharacterSpanList': [[2526, 2552]],", 
        "  u'RawText': u\"they're ruining the market\",", 
        "  u'TokenList': [[2526, 2530, 472, 15, 15],", 
        "   [2530, 2533, 473, 15, 16],", 
        "   [2534, 2541, 474, 15, 17],", 
        "   [2542, 2545, 475, 15, 18],", 
        "   [2546, 2552, 476, 15, 19]]},", 
        " u'Connective': {u'CharacterSpanList': [[2518, 2525]],", 
        "  u'RawText': u'because',", 
        "  u'TokenList': [[2518, 2525, 471, 15, 14]]},", 
        " u'DocID': u'wsj_1000',", 
        " u'ID': 15007,", 
        " u'Sense': [u'Contingency.Cause.Reason'],", 
        " u'Type': u'Explicit'}"
       ]
      }
     ], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "How do I get the constituent parse?", 
      "--------------------------", 
      "The automatic parses and part-of-speech tags are provided in a separate json file.", 
      "Note that this file contains only one line unlike the PDTB json file.", 
      "Suppose we want the parse for the sentence in the relation above, which is sentence #15 shown in `TokenList`."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "parse_file = open('pdtb_trial_parses.json')", 
      "parse_dict = json.loads(parse_file.read())", 
      "doc_id = example_relation['DocID']", 
      "", 
      "parse_dict[doc_id]['sentences'][15]['parsetree']"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 2, 
       "text": [
        "u\"( (S (NP (PRP We)) (VP (VBP 've) (VP (VP (VBN talked) (PP (TO to) (NP (NP (NNS proponents)) (PP (IN of) (NP (NN index) (NN arbitrage)))))) (CC and) (VP (VBD told) (NP (PRP them)) (S (VP (TO to) (VP (VB cool) (NP (PRP it)) (SBAR (IN because) (S (NP (PRP they)) (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market)))))))))))) (. .)) )\\n\""
       ]
      }
     ], 
     "prompt_number": 2
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "How do I get the dependency parse?", 
      "-------------------", 
      "A list of dependencies for the sentence is in the same dictionary. And it is in the form of (dependency type, HEAD token-token position, DEPENDENT token-token position)"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "parse_dict[doc_id]['sentences'][15]['dependencies']"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 3, 
       "text": [
        "[[u'nsubj', u'talked-3', u'We-1'],", 
        " [u'aux', u'talked-3', u\"'ve-2\"],", 
        " [u'root', u'ROOT-0', u'talked-3'],", 
        " [u'prep', u'talked-3', u'to-4'],", 
        " [u'pobj', u'to-4', u'proponents-5'],", 
        " [u'prep', u'proponents-5', u'of-6'],", 
        " [u'nn', u'arbitrage-8', u'index-7'],", 
        " [u'pobj', u'of-6', u'arbitrage-8'],", 
        " [u'cc', u'talked-3', u'and-9'],", 
        " [u'conj', u'talked-3', u'told-10'],", 
        " [u'dobj', u'told-10', u'them-11'],", 
        " [u'aux', u'cool-13', u'to-12'],", 
        " [u'xcomp', u'told-10', u'cool-13'],", 
        " [u'dobj', u'cool-13', u'it-14'],", 
        " [u'mark', u'ruining-18', u'because-15'],", 
        " [u'nsubj', u'ruining-18', u'they-16'],", 
        " [u'aux', u'ruining-18', u\"'re-17\"],", 
        " [u'advcl', u'cool-13', u'ruining-18'],", 
        " [u'det', u'market-20', u'the-19'],", 
        " [u'dobj', u'ruining-18', u'market-20']]"
       ]
      }
     ], 
     "prompt_number": 3
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "How do I get part of speech tags?", 
      "---------------------", 
      "You can obviously get it from the parse tree but there is another way."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "parse_dict[doc_id]['sentences'][15]['words'][0]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 4, 
       "text": [
        "[u'We',", 
        " {u'CharacterOffsetBegin': 2447,", 
        "  u'CharacterOffsetEnd': 2449,", 
        "  u'Linkers': [u'arg2_15006', u'arg1_15008'],", 
        "  u'PartOfSpeech': u'PRP'}]"
       ]
      }
     ], 
     "prompt_number": 4
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "What about the next token? Increment the word index."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "parse_dict[doc_id]['sentences'][15]['words'][1]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 5, 
       "text": [
        "[u\"'ve\",", 
        " {u'CharacterOffsetBegin': 2449,", 
        "  u'CharacterOffsetEnd': 2452,", 
        "  u'Linkers': [u'arg2_15006', u'arg1_15008'],", 
        "  u'PartOfSpeech': u'VBP'}]"
       ]
      }
     ], 
     "prompt_number": 5
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "JSON is great and all. But I still like my CoNLL format. What can I do ?", 
      "--------------------------", 
      "JSON format makes your code much more readable instead of a bunch of unreadable indices. ", 
      "CoNLL format of this dataset is wicked sparse. Here's our suggested way to get something similar.", 
      "You can use the `Linker` field in each token dictionary. Here's an example."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "all_tokens = [token for sentence in parse_dict[doc_id]['sentences'] for token in sentence['words']]", 
      "for token in all_tokens[0:20]:", 
      "    for linker in token[1]['Linkers']:", 
      "        role, relation_id = linker.split('_')", 
      "        print '%s \\t is part of %s in relation id %s' % (token[0], role, relation_id)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "Kemper \t is part of arg1 in relation id 15010", 
        "Financial \t is part of arg1 in relation id 15010", 
        "Services \t is part of arg1 in relation id 15010", 
        "Inc. \t is part of arg1 in relation id 15010", 
        ", \t is part of arg1 in relation id 15010", 
        "charging \t is part of arg1 in relation id 15010", 
        "that \t is part of arg1 in relation id 15010", 
        "program \t is part of arg1 in relation id 15010", 
        "trading \t is part of arg1 in relation id 15010", 
        "is \t is part of arg1 in relation id 15010", 
        "ruining \t is part of arg1 in relation id 15010", 
        "the \t is part of arg1 in relation id 15010", 
        "stock \t is part of arg1 in relation id 15010", 
        "market \t is part of arg1 in relation id 15010", 
        ", \t is part of arg1 in relation id 15010", 
        "cut \t is part of arg1 in relation id 15010", 
        "off \t is part of arg1 in relation id 15010", 
        "four \t is part of arg1 in relation id 15010", 
        "big \t is part of arg1 in relation id 15010", 
        "Wall \t is part of arg1 in relation id 15010"
       ]
      }
     ], 
     "prompt_number": 6
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "We are provided CoNLL format of the data too, and I want to use that. What does each column mean?", 
      "------------------------------", 
      "In addition to token-level information (e.g. index, POS tag, etc), there are as many more columns as the number of discourse relations in the document. ", 
      "The CoNLL format of the data looks like this. Brace yourself. It's real ugly"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "conllx_format_data = open('wsj_1000.pdtb.conll').readlines()[0:10]", 
      "for line in conllx_format_data:", 
      "    print line"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "0\t0\t0\tKemper\tNNP\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "1\t0\t1\tFinancial\tNNP\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "2\t0\t2\tServices\tNNPS\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "3\t0\t3\tInc.\tNNP\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "4\t0\t4\t,\t,\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "5\t0\t5\tcharging\tVBG\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "6\t0\t6\tthat\tIN\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "7\t0\t7\tprogram\tNN\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "8\t0\t8\ttrading\tNN\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        "", 
        "9\t0\t9\tis\tVBZ\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_", 
        ""
       ]
      }
     ], 
     "prompt_number": 7
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Here's the explanation of each field if a document has n relations:", 
      "", 
      "* Document-level token index ", 
      "* Sentence index", 
      "* Sentence-level token index", 
      "* POS tag", 
      "* Relation 1 information", 
      "* Relation 2 information ", 
      "* ...", 
      "* Relation n information", 
      "", 
      "The relation information field can take many forms:", 
      "", 
      "* `arg1` part of Arg1 of the relation", 
      "* `arg2` part of Arg2 of the relation", 
      "* `conn|Comparison.Concession` part of the discourse connective AND the sense of that relation is Comparison.Concession (Explicit relations only)", 
      "* `arg2|EntRel` part of Arg2 of the relation AND the sense of that relation is EntRel (Entrel and Norel relations only)", 
      "* `arg2|because|Contingency.Pragmatic cause` part of Arg2 (Implicit relations only)", 
      "", 
      "What should the system output look like? ", 
      "-----------------------------------------------------", 
      "The system output must be in json format. It is very similar to the training set except for the `TokenList` field. ", 
      "The `TokenList` field is now a list of document level token indices.", 
      "If the relation is not explicit, `Connective` field must still be there, and its `TokenList` must be an empty list.", 
      "You may however add whatever field into json to help yourself debug or develop the system.", 
      "Below is an example of a relation given by a system."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "output_file = open('pdtb_trial_system_output.json')", 
      "output_relations = [json.loads(x) for x in output_file]", 
      "output_relations[7]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 15, 
       "text": [
        "{u'Arg1': {u'TokenList': [443,", 
        "   444,", 
        "   445,", 
        "   446,", 
        "   447,", 
        "   448,", 
        "   449,", 
        "   450,", 
        "   451,", 
        "   452,", 
        "   453,", 
        "   454,", 
        "   455]},", 
        " u'Arg2': {u'TokenList': [422,", 
        "   423,", 
        "   424,", 
        "   425,", 
        "   426,", 
        "   427,", 
        "   428,", 
        "   429,", 
        "   430,", 
        "   431,", 
        "   432,", 
        "   433,", 
        "   434,", 
        "   435,", 
        "   436,", 
        "   437,", 
        "   438,", 
        "   439,", 
        "   440]},", 
        " u'Connective': {u'TokenList': [421]},", 
        " u'DocID': u'wsj_1000',", 
        " u'Sense': [u'Comparison.Concession'],", 
        " u'Type': u'Explicit'}"
       ]
      }
     ], 
     "prompt_number": 15
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Validator and scorer", 
      "----------------------------", 
      "Suppose you already have a system and you want to evaluate the system. ", 
      "We provide `validator.py` and `scorer.py` to help you validate the format of the system out and evaluate the system respectively. ", 
      "These utility functions can be downloaded from [CoNLL Shared Task Github](www.github.com/attapol/conll15st).", 
      "The usage is included in the functions. ", 
      "", 
      "That should be all that you need! Let's get the fun started.", 
      "----------------------------", 
      "If you find any errors or suggestions, please post to the forum or email the organizing committee at `conll15st@gmail.com`. ", 
      "We hope you enjoy solving this challenging task of shallow discourse parsing. ", 
      "Together, we can make progress in understanding discourse phenomena."
     ]
    }
   ]
  }
 ]
}